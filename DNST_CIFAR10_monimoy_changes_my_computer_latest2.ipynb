{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/monimoyd/DNST_CIFAR10/blob/master/DNST_CIFAR10_monimoy_changes_1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7tCV4NJbPXI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "notebook 5.4.0 requires ipykernel, which is not installed.\n",
      "jupyter 1.0.0 requires ipykernel, which is not installed.\n",
      "jupyter-console 5.2.0 requires ipykernel, which is not installed.\n",
      "ipywidgets 7.1.1 requires ipykernel>=4.5.1, which is not installed.\n",
      "tensorflow-gpu 1.9.0 has requirement tensorboard<1.10.0,>=1.9.0, but you'll have tensorboard 1.10.0 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
      "E:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/\n",
    "!pip install -q keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_DN4vvW7d-Jx"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_D7He0K8eKqQ"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_x2JuqDceTUG"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 250\n",
    "l = 5\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "P9oAHIKLenCA",
    "outputId": "4d4e3790-8d15-4add-ee79-d1393ad8a2cf"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Rotate images by 45 degrees\n",
    "datagen1 = ImageDataGenerator(rotation_range=45)\n",
    "\n",
    "# fit parameters from data\n",
    "datagen1.fit(x_train)\n",
    "\n",
    "# Get 1000 augmented images and append to the training data\n",
    "for x_rotated_images_batch, y_rotated_images_batch in datagen1.flow(x_train, y_train, batch_size=1000):\n",
    "    x_train = np.append(x_train, x_rotated_images_batch, axis=0)\n",
    "    y_train = np.append(y_train, y_rotated_images_batch, axis=0)\n",
    "    break\n",
    "\n",
    "# Horizontally flip image\n",
    "datagen2 = ImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "# fit parameters from data\n",
    "datagen2.fit(x_train)  \n",
    "\n",
    "# Get 1000 augmented horizontally flipped images and append to the training data\n",
    "for x_flipped_images_batch, y_flipped_images_batch in datagen2.flow(x_train, y_train, batch_size=1000):\n",
    "    x_train = np.append(x_train, x_flipped_images_batch, axis=0)\n",
    "    y_train = np.append(y_train, y_flipped_images_batch, axis=0)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "epgz2n0ve48o"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        Conv2D_1_1 = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3),  use_bias=False ,padding='same')(relu)\n",
    "        Conv2D_5_5 = Conv2D(int(num_filter*compression), (5,5),  use_bias=False ,padding='same')(relu)\n",
    "        Conv2D_7_7 = Conv2D(int(num_filter*compression), (7,7),  use_bias=False ,padding='same')(relu)\n",
    "        Conv2D_all_filters = Concatenate(axis=-1)([Conv2D_1_1,Conv2D_3_3,Conv2D_5_5,Conv2D_7_7])\n",
    "        if dropout_rate>0:\n",
    "          Conv2D_all_filters = Dropout(dropout_rate)(Conv2D_all_filters)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_all_filters])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SV4_k7hzfF-C"
   },
   "outputs": [],
   "source": [
    "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    \n",
    "    return AvgPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qvy4cIPPfKo1"
   },
   "outputs": [],
   "source": [
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zK2f9RdpfWRy"
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "layer1_transition = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "layer2_block = add_denseblock(layer1_transition, num_filter, dropout_rate)\n",
    "layer2_transition = add_transition(layer2_block, num_filter, dropout_rate)\n",
    "skip_connection1 = layer2_transition\n",
    "\n",
    "layer3_block = add_denseblock(layer2_transition, num_filter, dropout_rate)\n",
    "layer3_transition = add_transition(layer3_block, num_filter, dropout_rate)\n",
    "\n",
    "layer4_block = add_denseblock(layer3_transition, num_filter, dropout_rate)\n",
    "layer4_transition = add_transition(layer4_block, num_filter, dropout_rate)\n",
    "\n",
    "layer5_skip_connection_block = add_denseblock(skip_connection1, num_filter, dropout_rate)\n",
    "layer5_skip_connection_block = Conv2D(int(num_filter*compression), (5,5), use_bias=False)(layer5_skip_connection_block)\n",
    "layer5_skip_connection_block = Conv2D(int(num_filter*compression), (5,5), use_bias=False)(layer5_skip_connection_block)\n",
    "layer5_skip_connection_block = Conv2D(int(num_filter*compression), (5,5), use_bias=False)(layer5_skip_connection_block)\n",
    "layer5_block = Concatenate(axis=-1)([layer5_skip_connection_block, layer4_transition])\n",
    "layer5_transition = add_transition(layer5_block, num_filter, dropout_rate)\n",
    "\n",
    "layer6_block = add_denseblock(layer5_transition,  num_filter, dropout_rate)\n",
    "output = output_layer(layer6_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10564
    },
    "colab_type": "code",
    "id": "Ur0VMkKKfiMs",
    "outputId": "e2f9e2bb-4455-4348-f4e5-82a496b672ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 12)   324         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 12)   48          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 12)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 6)    72          activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 6)    648         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 6)    1800        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 6)    3528        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 24)   0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 24)   0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 36)   0           conv2d_1[0][0]                   \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 36)   144         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 36)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 6)    216         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 6)    1944        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 6)    5400        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 6)    10584       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 24)   0           conv2d_6[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 24)   0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 60)   0           concatenate_2[0][0]              \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 60)   240         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 60)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 6)    360         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 6)    3240        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 6)    9000        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 6)    17640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 24)   0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 24)   0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 84)   0           concatenate_4[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 84)   336         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 84)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 6)    504         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 6)    4536        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 6)    12600       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 6)    24696       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 24)   0           conv2d_14[0][0]                  \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 24)   0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 108)  0           concatenate_6[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 108)  432         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 108)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 6)    648         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 6)    5832        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 6)    16200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 6)    31752       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 24)   0           conv2d_18[0][0]                  \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 24)   0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 132)  0           concatenate_8[0][0]              \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 132)  528         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 132)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 6)    792         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 6)    0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 6)    0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 6)    24          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 6)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 6)    36          activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 6)    324         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 6)    900         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 6)    1764        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 24)   0           conv2d_23[0][0]                  \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 16, 16, 24)   0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 30)   0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 30)   120         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 30)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 6)    180         activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 6)    1620        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 6)    4500        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 6)    8820        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 24)   0           conv2d_27[0][0]                  \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 16, 16, 24)   0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 16, 16, 54)   0           concatenate_12[0][0]             \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 54)   216         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 54)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 6)    324         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 6)    2916        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 6)    8100        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 6)    15876       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 24)   0           conv2d_31[0][0]                  \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 16, 16, 24)   0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 78)   0           concatenate_14[0][0]             \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 78)   312         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 78)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 6)    468         activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 6)    4212        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 16, 16, 6)    11700       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 16, 16, 6)    22932       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 24)   0           conv2d_35[0][0]                  \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 16, 16, 24)   0           concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 102)  0           concatenate_16[0][0]             \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 102)  408         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 102)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 16, 16, 6)    612         activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 6)    5508        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 16, 6)    15300       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 16, 16, 6)    29988       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 24)   0           conv2d_39[0][0]                  \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "                                                                 conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16, 16, 24)   0           concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 126)  0           concatenate_18[0][0]             \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 126)  504         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 126)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 16, 16, 6)    756         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16, 16, 6)    0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 8, 8, 6)      0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 6)      24          average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 6)      0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 6)    24          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 6)      36          activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 8, 8, 6)      324         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 8, 8, 6)      900         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 8, 8, 6)      1764        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 6)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 8, 8, 24)     0           conv2d_44[0][0]                  \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "                                                                 conv2d_46[0][0]                  \n",
      "                                                                 conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 16, 6)    36          activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 6)    324         activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 6)    900         activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 16, 16, 6)    1764        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 8, 8, 24)     0           concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 16, 16, 24)   0           conv2d_65[0][0]                  \n",
      "                                                                 conv2d_66[0][0]                  \n",
      "                                                                 conv2d_67[0][0]                  \n",
      "                                                                 conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 8, 8, 30)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 16, 16, 24)   0           concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 30)     120         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 16, 16, 30)   0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 30)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 30)   120         concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 8, 8, 6)      180         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 8, 8, 6)      1620        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 8, 8, 6)      4500        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 8, 8, 6)      8820        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 30)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 8, 8, 24)     0           conv2d_48[0][0]                  \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "                                                                 conv2d_50[0][0]                  \n",
      "                                                                 conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 16, 16, 6)    180         activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 16, 16, 6)    1620        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 16, 16, 6)    4500        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 16, 16, 6)    8820        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 8, 8, 24)     0           concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 16, 16, 24)   0           conv2d_69[0][0]                  \n",
      "                                                                 conv2d_70[0][0]                  \n",
      "                                                                 conv2d_71[0][0]                  \n",
      "                                                                 conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 8, 8, 54)     0           concatenate_22[0][0]             \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 16, 16, 24)   0           concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 54)     216         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 16, 16, 54)   0           concatenate_32[0][0]             \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 54)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 54)   216         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 8, 8, 6)      324         activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 8, 8, 6)      2916        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 8, 8, 6)      8100        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 8, 8, 6)      15876       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 54)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 24)     0           conv2d_52[0][0]                  \n",
      "                                                                 conv2d_53[0][0]                  \n",
      "                                                                 conv2d_54[0][0]                  \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 16, 16, 6)    324         activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 16, 16, 6)    2916        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 16, 16, 6)    8100        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 16, 16, 6)    15876       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 8, 8, 24)     0           concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 16, 16, 24)   0           conv2d_73[0][0]                  \n",
      "                                                                 conv2d_74[0][0]                  \n",
      "                                                                 conv2d_75[0][0]                  \n",
      "                                                                 conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 8, 8, 78)     0           concatenate_24[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 16, 16, 24)   0           concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 78)     312         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 16, 16, 78)   0           concatenate_34[0][0]             \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 78)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 78)   312         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 8, 8, 6)      468         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 8, 8, 6)      4212        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 8, 8, 6)      11700       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 8, 8, 6)      22932       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 78)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 8, 8, 24)     0           conv2d_56[0][0]                  \n",
      "                                                                 conv2d_57[0][0]                  \n",
      "                                                                 conv2d_58[0][0]                  \n",
      "                                                                 conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 16, 16, 6)    468         activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 16, 16, 6)    4212        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 16, 16, 6)    11700       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 16, 16, 6)    22932       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 8, 8, 24)     0           concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 16, 16, 24)   0           conv2d_77[0][0]                  \n",
      "                                                                 conv2d_78[0][0]                  \n",
      "                                                                 conv2d_79[0][0]                  \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 8, 8, 102)    0           concatenate_26[0][0]             \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 16, 16, 24)   0           concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 102)    408         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 16, 16, 102)  0           concatenate_36[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 102)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 102)  408         concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 8, 8, 6)      612         activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 8, 8, 6)      5508        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 8, 8, 6)      15300       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 8, 8, 6)      29988       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 102)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 8, 8, 24)     0           conv2d_60[0][0]                  \n",
      "                                                                 conv2d_61[0][0]                  \n",
      "                                                                 conv2d_62[0][0]                  \n",
      "                                                                 conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 16, 16, 6)    612         activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 16, 16, 6)    5508        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 16, 16, 6)    15300       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 16, 16, 6)    29988       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 8, 8, 24)     0           concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 16, 16, 24)   0           conv2d_81[0][0]                  \n",
      "                                                                 conv2d_82[0][0]                  \n",
      "                                                                 conv2d_83[0][0]                  \n",
      "                                                                 conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 8, 8, 126)    0           concatenate_28[0][0]             \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 16, 16, 24)   0           concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 126)    504         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 16, 16, 126)  0           concatenate_38[0][0]             \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 126)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 12, 12, 6)    18900       concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 8, 8, 6)      756         activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 6)      900         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 8, 8, 6)      0           conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 4, 4, 6)      900         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 4, 4, 6)      0           dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 4, 4, 12)     0           conv2d_87[0][0]                  \n",
      "                                                                 average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 4, 4, 12)     48          concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 12)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 4, 4, 6)      72          activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 4, 4, 6)      0           conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 2, 2, 6)      0           dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 2, 2, 6)      24          average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 2, 2, 6)      0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 2, 2, 6)      36          activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 2, 2, 6)      324         activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 2, 2, 6)      900         activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 2, 2, 6)      1764        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 2, 2, 24)     0           conv2d_89[0][0]                  \n",
      "                                                                 conv2d_90[0][0]                  \n",
      "                                                                 conv2d_91[0][0]                  \n",
      "                                                                 conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 2, 2, 24)     0           concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 2, 2, 30)     0           average_pooling2d_4[0][0]        \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 2, 2, 30)     120         concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 2, 2, 30)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 2, 2, 6)      180         activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 2, 2, 6)      1620        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 2, 2, 6)      4500        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 2, 2, 6)      8820        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 2, 2, 24)     0           conv2d_93[0][0]                  \n",
      "                                                                 conv2d_94[0][0]                  \n",
      "                                                                 conv2d_95[0][0]                  \n",
      "                                                                 conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 2, 2, 24)     0           concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 2, 2, 54)     0           concatenate_43[0][0]             \n",
      "                                                                 dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 2, 2, 54)     216         concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 2, 2, 54)     0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 2, 2, 6)      324         activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 2, 2, 6)      2916        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 2, 2, 6)      8100        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 2, 2, 6)      15876       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 2, 2, 24)     0           conv2d_97[0][0]                  \n",
      "                                                                 conv2d_98[0][0]                  \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "                                                                 conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 2, 2, 24)     0           concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 2, 2, 78)     0           concatenate_45[0][0]             \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 2, 2, 78)     312         concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 2, 2, 78)     0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 2, 2, 6)      468         activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 2, 2, 6)      4212        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 2, 2, 6)      11700       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 2, 2, 6)      22932       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 2, 2, 24)     0           conv2d_101[0][0]                 \n",
      "                                                                 conv2d_102[0][0]                 \n",
      "                                                                 conv2d_103[0][0]                 \n",
      "                                                                 conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 2, 2, 24)     0           concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 2, 2, 102)    0           concatenate_47[0][0]             \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 2, 2, 102)    408         concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 2, 2, 102)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 2, 2, 6)      612         activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 2, 2, 6)      5508        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 2, 2, 6)      15300       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 2, 2, 6)      29988       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 2, 2, 24)     0           conv2d_105[0][0]                 \n",
      "                                                                 conv2d_106[0][0]                 \n",
      "                                                                 conv2d_107[0][0]                 \n",
      "                                                                 conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 2, 2, 24)     0           concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 2, 2, 126)    0           concatenate_49[0][0]             \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 2, 2, 126)    504         concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 2, 2, 126)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 1, 1, 126)    0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 126)          0           average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           1270        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 727,798\n",
      "Trainable params: 723,994\n",
      "Non-trainable params: 3,804\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQMBfM7rf6IU"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks For Early Stopping and Saving the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "qXt6uF-ZgEtG",
    "outputId": "17c39cad-e24a-40ba-d95c-05acd5777dda",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class EarlyStoppingByValidationAccuracy(Callback):\n",
    "    def __init__(self, monitor='val_acc', value=1.0, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        accuracy_value= logs.get(self.monitor)\n",
    "       \n",
    "        if accuracy_value >= self.value:\n",
    "            self.model.stop_training = True\n",
    "            if self.verbose == 1:\n",
    "                print(\"Epoch %d: Threshold for early stopping has reached\" % (epoch + 1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks_list=[]\n",
    "model_save_path= \"best_model-CIFAR10-monimoy-my_computer_latest2.h5\"\n",
    "callbacks_list.append(EarlyStoppingByValidationAccuracy(monitor='val_acc',  value=0.9201, verbose=1))\n",
    "callbacks_list.append(ModelCheckpoint(model_save_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "52000/52000 [==============================] - 189s 4ms/step - loss: 1.9231 - acc: 0.2633 - val_loss: 1.7646 - val_acc: 0.3311\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.33110, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 2/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 1.6160 - acc: 0.3847 - val_loss: 1.5035 - val_acc: 0.4367\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.33110 to 0.43670, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 3/250\n",
      "52000/52000 [==============================] - 170s 3ms/step - loss: 1.4753 - acc: 0.4528 - val_loss: 1.5222 - val_acc: 0.4533\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.43670 to 0.45330, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 4/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 1.3677 - acc: 0.4972 - val_loss: 1.4259 - val_acc: 0.5003\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.45330 to 0.50030, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 5/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 1.2864 - acc: 0.5330 - val_loss: 1.5301 - val_acc: 0.4497\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.50030\n",
      "Epoch 6/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 1.2234 - acc: 0.5587 - val_loss: 5.2769 - val_acc: 0.1874\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.50030\n",
      "Epoch 7/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 1.1622 - acc: 0.5834 - val_loss: 1.5395 - val_acc: 0.4947\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.50030\n",
      "Epoch 8/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 1.1123 - acc: 0.6045 - val_loss: 2.1890 - val_acc: 0.4038\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.50030\n",
      "Epoch 9/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 1.0692 - acc: 0.6196 - val_loss: 1.5857 - val_acc: 0.5289\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.50030 to 0.52890, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 10/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 1.0254 - acc: 0.6368 - val_loss: 1.0988 - val_acc: 0.6207\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.52890 to 0.62070, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 11/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.9915 - acc: 0.6499 - val_loss: 1.0183 - val_acc: 0.6470\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.62070 to 0.64700, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 12/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.9555 - acc: 0.6624 - val_loss: 1.0200 - val_acc: 0.6540\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.64700 to 0.65400, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 13/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.9319 - acc: 0.6716 - val_loss: 1.1909 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.65400\n",
      "Epoch 14/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.9037 - acc: 0.6833 - val_loss: 1.1256 - val_acc: 0.6263\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.65400\n",
      "Epoch 15/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.8757 - acc: 0.6929 - val_loss: 0.9459 - val_acc: 0.6764\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.65400 to 0.67640, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 16/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.8579 - acc: 0.6971 - val_loss: 0.9156 - val_acc: 0.6861\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.67640 to 0.68610, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 17/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 0.8325 - acc: 0.7097 - val_loss: 0.9246 - val_acc: 0.6852\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.68610\n",
      "Epoch 18/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.8140 - acc: 0.7160 - val_loss: 0.8714 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.68610 to 0.70830, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 19/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.7922 - acc: 0.7243 - val_loss: 1.2250 - val_acc: 0.6246\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.70830\n",
      "Epoch 20/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.7771 - acc: 0.7297 - val_loss: 0.9923 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.70830\n",
      "Epoch 21/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.7628 - acc: 0.7337 - val_loss: 0.8805 - val_acc: 0.7167\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.70830 to 0.71670, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 22/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.7460 - acc: 0.7391 - val_loss: 1.0975 - val_acc: 0.6510\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.71670\n",
      "Epoch 23/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 0.7269 - acc: 0.7464 - val_loss: 0.8309 - val_acc: 0.7264\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.71670 to 0.72640, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 24/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 0.7179 - acc: 0.7503 - val_loss: 0.9227 - val_acc: 0.7076\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.72640\n",
      "Epoch 25/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.7046 - acc: 0.7572 - val_loss: 0.8624 - val_acc: 0.7210\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.72640\n",
      "Epoch 26/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.6901 - acc: 0.7604 - val_loss: 0.8366 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.72640 to 0.73120, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 27/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.6813 - acc: 0.7652 - val_loss: 0.9232 - val_acc: 0.7175\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.73120\n",
      "Epoch 28/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.6663 - acc: 0.7689 - val_loss: 0.8975 - val_acc: 0.7149\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.73120\n",
      "Epoch 29/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.6527 - acc: 0.7753 - val_loss: 0.9419 - val_acc: 0.7193\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.73120\n",
      "Epoch 30/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.6517 - acc: 0.7746 - val_loss: 0.7288 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.73120 to 0.75340, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 31/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.6364 - acc: 0.7800 - val_loss: 1.0614 - val_acc: 0.6814\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.75340\n",
      "Epoch 32/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.6211 - acc: 0.7851 - val_loss: 0.8423 - val_acc: 0.7363\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.75340\n",
      "Epoch 33/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.6129 - acc: 0.7888 - val_loss: 0.8544 - val_acc: 0.7257\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.75340\n",
      "Epoch 34/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.6126 - acc: 0.7887 - val_loss: 0.7678 - val_acc: 0.7590\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.75340 to 0.75900, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 35/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 0.5986 - acc: 0.7953 - val_loss: 0.7657 - val_acc: 0.7563\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.75900\n",
      "Epoch 36/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 0.5903 - acc: 0.7968 - val_loss: 0.8066 - val_acc: 0.7496\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.75900\n",
      "Epoch 37/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.5826 - acc: 0.7995 - val_loss: 0.7128 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.75900 to 0.77300, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.5739 - acc: 0.8016 - val_loss: 0.7409 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.77300\n",
      "Epoch 39/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.5670 - acc: 0.8045 - val_loss: 0.8662 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.77300\n",
      "Epoch 40/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.5602 - acc: 0.8067 - val_loss: 0.7324 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.77300\n",
      "Epoch 41/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 0.5556 - acc: 0.8087 - val_loss: 0.7711 - val_acc: 0.7590\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.77300\n",
      "Epoch 42/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.5483 - acc: 0.8126 - val_loss: 0.7930 - val_acc: 0.7620\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.77300\n",
      "Epoch 43/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.5398 - acc: 0.8143 - val_loss: 0.7439 - val_acc: 0.7662\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.77300\n",
      "Epoch 44/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.5324 - acc: 0.8169 - val_loss: 0.7363 - val_acc: 0.7755\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.77300 to 0.77550, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 45/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.5297 - acc: 0.8181 - val_loss: 0.7427 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.77550\n",
      "Epoch 46/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.5210 - acc: 0.8217 - val_loss: 0.6790 - val_acc: 0.7816\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.77550 to 0.78160, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 47/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.5170 - acc: 0.8208 - val_loss: 0.7310 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.78160\n",
      "Epoch 48/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.5110 - acc: 0.8236 - val_loss: 0.6647 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.78160 to 0.79400, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 49/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.5040 - acc: 0.8277 - val_loss: 0.7096 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.79400\n",
      "Epoch 50/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.5008 - acc: 0.8281 - val_loss: 0.6896 - val_acc: 0.7883\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.79400\n",
      "Epoch 51/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.4922 - acc: 0.8322 - val_loss: 0.7909 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.79400\n",
      "Epoch 52/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.4913 - acc: 0.8316 - val_loss: 0.6613 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.79400\n",
      "Epoch 53/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.4888 - acc: 0.8324 - val_loss: 0.8501 - val_acc: 0.7548\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.79400\n",
      "Epoch 54/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.4784 - acc: 0.8372 - val_loss: 0.7010 - val_acc: 0.7825\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.79400\n",
      "Epoch 55/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.4719 - acc: 0.8378 - val_loss: 0.8773 - val_acc: 0.7557\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.79400\n",
      "Epoch 56/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.4671 - acc: 0.8401 - val_loss: 0.6360 - val_acc: 0.8028\n",
      "\n",
      "Epoch 00056: val_acc improved from 0.79400 to 0.80280, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 57/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.4625 - acc: 0.8409 - val_loss: 0.7249 - val_acc: 0.7859\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.80280\n",
      "Epoch 58/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.4602 - acc: 0.8409 - val_loss: 0.8533 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.80280\n",
      "Epoch 59/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.4533 - acc: 0.8432 - val_loss: 0.7973 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.80280\n",
      "Epoch 60/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.4510 - acc: 0.8461 - val_loss: 0.7459 - val_acc: 0.7820\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.80280\n",
      "Epoch 61/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.4455 - acc: 0.8474 - val_loss: 0.8970 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.80280\n",
      "Epoch 62/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.4468 - acc: 0.8454 - val_loss: 0.6682 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.80280\n",
      "Epoch 63/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.4406 - acc: 0.8481 - val_loss: 0.7411 - val_acc: 0.7823\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.80280\n",
      "Epoch 64/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.4338 - acc: 0.8520 - val_loss: 0.6675 - val_acc: 0.8067\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.80280 to 0.80670, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 65/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 0.4334 - acc: 0.8504 - val_loss: 0.7931 - val_acc: 0.7706\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.80670\n",
      "Epoch 66/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.4308 - acc: 0.8507 - val_loss: 0.7132 - val_acc: 0.7905\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.80670\n",
      "Epoch 67/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.4257 - acc: 0.8543 - val_loss: 0.8040 - val_acc: 0.7769\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.80670\n",
      "Epoch 68/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.4231 - acc: 0.8535 - val_loss: 0.6597 - val_acc: 0.8066\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.80670\n",
      "Epoch 69/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.4160 - acc: 0.8570 - val_loss: 0.6735 - val_acc: 0.7957\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.80670\n",
      "Epoch 70/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.4137 - acc: 0.8575 - val_loss: 0.6384 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.80670\n",
      "Epoch 71/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.4089 - acc: 0.8590 - val_loss: 0.7838 - val_acc: 0.7823\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.80670\n",
      "Epoch 72/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.4061 - acc: 0.8597 - val_loss: 0.7539 - val_acc: 0.7828\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.80670\n",
      "Epoch 73/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.4085 - acc: 0.8589 - val_loss: 0.7192 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.80670\n",
      "Epoch 74/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.4015 - acc: 0.8614 - val_loss: 0.8997 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.80670\n",
      "Epoch 75/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3934 - acc: 0.8658 - val_loss: 0.7305 - val_acc: 0.7873\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.80670\n",
      "Epoch 76/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3918 - acc: 0.8648 - val_loss: 0.7152 - val_acc: 0.7961\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.80670\n",
      "Epoch 77/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3923 - acc: 0.8631 - val_loss: 0.6959 - val_acc: 0.7982\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.80670\n",
      "Epoch 78/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3869 - acc: 0.8662 - val_loss: 0.7097 - val_acc: 0.7988\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.80670\n",
      "Epoch 79/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3826 - acc: 0.8673 - val_loss: 0.6002 - val_acc: 0.8234\n",
      "\n",
      "Epoch 00079: val_acc improved from 0.80670 to 0.82340, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 80/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 0.3835 - acc: 0.8689 - val_loss: 0.7053 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.82340\n",
      "Epoch 81/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.3784 - acc: 0.8699 - val_loss: 0.7243 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.82340\n",
      "Epoch 82/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.3746 - acc: 0.8716 - val_loss: 0.7744 - val_acc: 0.7882\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.82340\n",
      "Epoch 83/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.3705 - acc: 0.8727 - val_loss: 0.8590 - val_acc: 0.7602\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.82340\n",
      "Epoch 84/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3724 - acc: 0.8721 - val_loss: 0.6411 - val_acc: 0.8131\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.82340\n",
      "Epoch 85/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3683 - acc: 0.8749 - val_loss: 0.7355 - val_acc: 0.7942\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.82340\n",
      "Epoch 86/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3672 - acc: 0.8724 - val_loss: 0.8174 - val_acc: 0.7813\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.82340\n",
      "Epoch 87/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3631 - acc: 0.8747 - val_loss: 0.7917 - val_acc: 0.7867\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.82340\n",
      "Epoch 88/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3548 - acc: 0.8761 - val_loss: 0.6056 - val_acc: 0.8256\n",
      "\n",
      "Epoch 00088: val_acc improved from 0.82340 to 0.82560, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 89/250\n",
      "52000/52000 [==============================] - 175s 3ms/step - loss: 0.3552 - acc: 0.8775 - val_loss: 0.6946 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.82560\n",
      "Epoch 90/250\n",
      "52000/52000 [==============================] - 175s 3ms/step - loss: 0.3539 - acc: 0.8781 - val_loss: 0.7372 - val_acc: 0.7905\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.82560\n",
      "Epoch 91/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3527 - acc: 0.8773 - val_loss: 0.7265 - val_acc: 0.8002\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.82560\n",
      "Epoch 92/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3480 - acc: 0.8801 - val_loss: 0.6982 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.82560\n",
      "Epoch 93/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.3492 - acc: 0.8796 - val_loss: 0.6879 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.82560\n",
      "Epoch 94/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.3392 - acc: 0.8813 - val_loss: 0.7574 - val_acc: 0.7967\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.82560\n",
      "Epoch 95/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3415 - acc: 0.8834 - val_loss: 0.7443 - val_acc: 0.7993\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.82560\n",
      "Epoch 96/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3436 - acc: 0.8802 - val_loss: 0.7008 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.82560\n",
      "Epoch 97/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3387 - acc: 0.8842 - val_loss: 0.6904 - val_acc: 0.8081\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.82560\n",
      "Epoch 98/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3326 - acc: 0.8853 - val_loss: 0.6584 - val_acc: 0.8189\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.82560\n",
      "Epoch 99/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3312 - acc: 0.8858 - val_loss: 0.6947 - val_acc: 0.8091\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.82560\n",
      "Epoch 100/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.3320 - acc: 0.8855 - val_loss: 0.6694 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.82560\n",
      "Epoch 101/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.3252 - acc: 0.8862 - val_loss: 0.8109 - val_acc: 0.7793\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.82560\n",
      "Epoch 102/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.3262 - acc: 0.8878 - val_loss: 0.6520 - val_acc: 0.8172\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.82560\n",
      "Epoch 103/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.3256 - acc: 0.8874 - val_loss: 0.6892 - val_acc: 0.8103\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.82560\n",
      "Epoch 104/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.3240 - acc: 0.8878 - val_loss: 0.8020 - val_acc: 0.7886\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.82560\n",
      "Epoch 105/250\n",
      "52000/52000 [==============================] - 175s 3ms/step - loss: 0.3154 - acc: 0.8908 - val_loss: 0.6609 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.82560\n",
      "Epoch 106/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.3212 - acc: 0.8874 - val_loss: 0.7320 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.82560\n",
      "Epoch 107/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.3166 - acc: 0.8904 - val_loss: 0.7077 - val_acc: 0.8026\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.82560\n",
      "Epoch 108/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.3128 - acc: 0.8901 - val_loss: 0.7218 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.82560\n",
      "Epoch 109/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.3096 - acc: 0.8910 - val_loss: 0.9415 - val_acc: 0.7566\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.82560\n",
      "Epoch 110/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.3077 - acc: 0.8934 - val_loss: 0.6774 - val_acc: 0.8153\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.82560\n",
      "Epoch 111/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.3085 - acc: 0.8924 - val_loss: 0.6419 - val_acc: 0.8235\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.82560\n",
      "Epoch 112/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3019 - acc: 0.8959 - val_loss: 0.7248 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.82560\n",
      "Epoch 113/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3045 - acc: 0.8942 - val_loss: 0.7637 - val_acc: 0.7968\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.82560\n",
      "Epoch 114/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.3066 - acc: 0.8938 - val_loss: 0.7373 - val_acc: 0.8062\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.82560\n",
      "Epoch 115/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2973 - acc: 0.8962 - val_loss: 0.7686 - val_acc: 0.7943\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.82560\n",
      "Epoch 116/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2943 - acc: 0.8973 - val_loss: 0.7051 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.82560\n",
      "Epoch 117/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2934 - acc: 0.8985 - val_loss: 0.6721 - val_acc: 0.8183\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.82560\n",
      "Epoch 118/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2920 - acc: 0.8986 - val_loss: 0.6763 - val_acc: 0.8192\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.82560\n",
      "Epoch 119/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2904 - acc: 0.8987 - val_loss: 0.6993 - val_acc: 0.8138\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.82560\n",
      "Epoch 120/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2911 - acc: 0.8996 - val_loss: 0.7040 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.82560\n",
      "Epoch 121/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2898 - acc: 0.8983 - val_loss: 0.7315 - val_acc: 0.8097\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.82560\n",
      "Epoch 122/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2891 - acc: 0.8998 - val_loss: 0.7420 - val_acc: 0.8054\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.82560\n",
      "Epoch 123/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2848 - acc: 0.9008 - val_loss: 0.6619 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.82560\n",
      "Epoch 124/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2831 - acc: 0.9017 - val_loss: 0.6994 - val_acc: 0.8135\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.82560\n",
      "Epoch 125/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2838 - acc: 0.9020 - val_loss: 0.8486 - val_acc: 0.7849\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.82560\n",
      "Epoch 126/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2812 - acc: 0.9029 - val_loss: 0.7004 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.82560\n",
      "Epoch 127/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2793 - acc: 0.9038 - val_loss: 0.6672 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.82560\n",
      "Epoch 128/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2746 - acc: 0.9043 - val_loss: 0.6618 - val_acc: 0.8249\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.82560\n",
      "Epoch 129/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2699 - acc: 0.9054 - val_loss: 0.6451 - val_acc: 0.8266\n",
      "\n",
      "Epoch 00129: val_acc improved from 0.82560 to 0.82660, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 130/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2744 - acc: 0.9040 - val_loss: 0.7038 - val_acc: 0.8158\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.82660\n",
      "Epoch 131/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2663 - acc: 0.9072 - val_loss: 0.6679 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.82660\n",
      "Epoch 132/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2636 - acc: 0.9086 - val_loss: 0.6158 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00132: val_acc improved from 0.82660 to 0.83650, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 133/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2671 - acc: 0.9073 - val_loss: 0.6957 - val_acc: 0.8187\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.83650\n",
      "Epoch 134/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 0.2660 - acc: 0.9075 - val_loss: 0.6761 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.83650\n",
      "Epoch 135/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 0.2628 - acc: 0.9082 - val_loss: 0.7400 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.83650\n",
      "Epoch 136/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2647 - acc: 0.9066 - val_loss: 0.6737 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.83650\n",
      "Epoch 137/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2595 - acc: 0.9104 - val_loss: 0.7579 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.83650\n",
      "Epoch 138/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2559 - acc: 0.9114 - val_loss: 0.6786 - val_acc: 0.8269\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.83650\n",
      "Epoch 139/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2551 - acc: 0.9112 - val_loss: 0.7071 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.83650\n",
      "Epoch 140/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2562 - acc: 0.9108 - val_loss: 0.7206 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.83650\n",
      "Epoch 141/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2574 - acc: 0.9110 - val_loss: 0.7844 - val_acc: 0.8056\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.83650\n",
      "Epoch 142/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2591 - acc: 0.9099 - val_loss: 0.6793 - val_acc: 0.8242\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.83650\n",
      "Epoch 143/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2508 - acc: 0.9138 - val_loss: 0.7129 - val_acc: 0.8187\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.83650\n",
      "Epoch 144/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2466 - acc: 0.9156 - val_loss: 0.6913 - val_acc: 0.8259\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.83650\n",
      "Epoch 145/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2479 - acc: 0.9129 - val_loss: 0.7165 - val_acc: 0.8214\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.83650\n",
      "Epoch 146/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2461 - acc: 0.9146 - val_loss: 0.6996 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.83650\n",
      "Epoch 147/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2486 - acc: 0.9135 - val_loss: 0.7623 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.83650\n",
      "Epoch 148/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2486 - acc: 0.9140 - val_loss: 0.7151 - val_acc: 0.8172\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.83650\n",
      "Epoch 149/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2434 - acc: 0.9138 - val_loss: 0.6887 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.83650\n",
      "Epoch 150/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2479 - acc: 0.9131 - val_loss: 0.7139 - val_acc: 0.8194\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.83650\n",
      "Epoch 151/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2385 - acc: 0.9173 - val_loss: 0.6941 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.83650\n",
      "Epoch 152/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2434 - acc: 0.9158 - val_loss: 0.6445 - val_acc: 0.8337\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.83650\n",
      "Epoch 153/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2336 - acc: 0.9177 - val_loss: 0.6777 - val_acc: 0.8331\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.83650\n",
      "Epoch 154/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2357 - acc: 0.9168 - val_loss: 0.7481 - val_acc: 0.8179\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.83650\n",
      "Epoch 155/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2333 - acc: 0.9187 - val_loss: 0.6148 - val_acc: 0.8403\n",
      "\n",
      "Epoch 00155: val_acc improved from 0.83650 to 0.84030, saving model to best_model-CIFAR10-monimoy-my_computer_latest2.h5\n",
      "Epoch 156/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2342 - acc: 0.9174 - val_loss: 0.8732 - val_acc: 0.7864\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.84030\n",
      "Epoch 157/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2344 - acc: 0.9177 - val_loss: 0.7317 - val_acc: 0.8202\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.84030\n",
      "Epoch 158/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2304 - acc: 0.9193 - val_loss: 0.8025 - val_acc: 0.8081\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.84030\n",
      "Epoch 159/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2345 - acc: 0.9185 - val_loss: 0.7516 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.84030\n",
      "Epoch 160/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2324 - acc: 0.9189 - val_loss: 0.6612 - val_acc: 0.8330\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.84030\n",
      "Epoch 161/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2308 - acc: 0.9190 - val_loss: 0.6999 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.84030\n",
      "Epoch 162/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2286 - acc: 0.9202 - val_loss: 0.7493 - val_acc: 0.8138\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.84030\n",
      "Epoch 163/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2269 - acc: 0.9208 - val_loss: 0.7402 - val_acc: 0.8195\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.84030\n",
      "Epoch 164/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2246 - acc: 0.9220 - val_loss: 0.7628 - val_acc: 0.8152\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.84030\n",
      "Epoch 165/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2262 - acc: 0.9206 - val_loss: 0.7819 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.84030\n",
      "Epoch 166/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2224 - acc: 0.9233 - val_loss: 0.7212 - val_acc: 0.8219\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.84030\n",
      "Epoch 167/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2208 - acc: 0.9230 - val_loss: 0.6786 - val_acc: 0.8346\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.84030\n",
      "Epoch 168/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2186 - acc: 0.9230 - val_loss: 0.6521 - val_acc: 0.8371\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.84030\n",
      "Epoch 169/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2201 - acc: 0.9227 - val_loss: 0.8558 - val_acc: 0.8003\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.84030\n",
      "Epoch 170/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2196 - acc: 0.9234 - val_loss: 0.6778 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.84030\n",
      "Epoch 171/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.2160 - acc: 0.9255 - val_loss: 0.7372 - val_acc: 0.8211\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.84030\n",
      "Epoch 172/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2166 - acc: 0.9239 - val_loss: 0.9162 - val_acc: 0.7886\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.84030\n",
      "Epoch 173/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2123 - acc: 0.9261 - val_loss: 0.7047 - val_acc: 0.8273\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.84030\n",
      "Epoch 174/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2163 - acc: 0.9241 - val_loss: 0.7968 - val_acc: 0.8138\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.84030\n",
      "Epoch 175/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2111 - acc: 0.9260 - val_loss: 0.6971 - val_acc: 0.8312\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.84030\n",
      "Epoch 176/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2113 - acc: 0.9253 - val_loss: 0.7466 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.84030\n",
      "Epoch 177/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2104 - acc: 0.9262 - val_loss: 0.7062 - val_acc: 0.8318\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.84030\n",
      "Epoch 178/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2100 - acc: 0.9263 - val_loss: 0.7704 - val_acc: 0.8254\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.84030\n",
      "Epoch 179/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2076 - acc: 0.9278 - val_loss: 0.7234 - val_acc: 0.8273\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.84030\n",
      "Epoch 180/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2058 - acc: 0.9293 - val_loss: 0.7747 - val_acc: 0.8189\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.84030\n",
      "Epoch 181/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2063 - acc: 0.9272 - val_loss: 0.8086 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.84030\n",
      "Epoch 182/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2040 - acc: 0.9282 - val_loss: 0.6838 - val_acc: 0.8396\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.84030\n",
      "Epoch 183/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2028 - acc: 0.9295 - val_loss: 0.7867 - val_acc: 0.8226\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.84030\n",
      "Epoch 184/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.1995 - acc: 0.9304 - val_loss: 0.8288 - val_acc: 0.8079\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.84030\n",
      "Epoch 185/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2010 - acc: 0.9304 - val_loss: 0.7459 - val_acc: 0.8225\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.84030\n",
      "Epoch 186/250\n",
      "52000/52000 [==============================] - 174s 3ms/step - loss: 0.2005 - acc: 0.9312 - val_loss: 0.7905 - val_acc: 0.8126\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.84030\n",
      "Epoch 187/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2007 - acc: 0.9305 - val_loss: 0.9560 - val_acc: 0.7922\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.84030\n",
      "Epoch 188/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.2012 - acc: 0.9292 - val_loss: 0.7068 - val_acc: 0.8318\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.84030\n",
      "Epoch 189/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1992 - acc: 0.9303 - val_loss: 0.7797 - val_acc: 0.8223\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.84030\n",
      "Epoch 190/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1984 - acc: 0.9307 - val_loss: 0.8524 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.84030\n",
      "Epoch 191/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1981 - acc: 0.9308 - val_loss: 0.7669 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.84030\n",
      "Epoch 192/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1938 - acc: 0.9328 - val_loss: 0.7249 - val_acc: 0.8288\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.84030\n",
      "Epoch 193/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1997 - acc: 0.9302 - val_loss: 0.7428 - val_acc: 0.8266\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.84030\n",
      "Epoch 194/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1961 - acc: 0.9328 - val_loss: 0.8001 - val_acc: 0.8189\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.84030\n",
      "Epoch 195/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 0.1914 - acc: 0.9319 - val_loss: 0.7611 - val_acc: 0.8210\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.84030\n",
      "Epoch 196/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1915 - acc: 0.9334 - val_loss: 0.7640 - val_acc: 0.8283\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.84030\n",
      "Epoch 197/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1888 - acc: 0.9336 - val_loss: 0.7284 - val_acc: 0.8296\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.84030\n",
      "Epoch 198/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1923 - acc: 0.9323 - val_loss: 0.8706 - val_acc: 0.8057\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.84030\n",
      "Epoch 199/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1856 - acc: 0.9356 - val_loss: 0.7476 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.84030\n",
      "Epoch 200/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1842 - acc: 0.9337 - val_loss: 0.7187 - val_acc: 0.8353\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.84030\n",
      "Epoch 201/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1909 - acc: 0.9336 - val_loss: 0.7352 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.84030\n",
      "Epoch 202/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1923 - acc: 0.9326 - val_loss: 0.7577 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.84030\n",
      "Epoch 203/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1838 - acc: 0.9372 - val_loss: 0.7686 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.84030\n",
      "Epoch 204/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1884 - acc: 0.9335 - val_loss: 0.7519 - val_acc: 0.8298\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.84030\n",
      "Epoch 205/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1796 - acc: 0.9380 - val_loss: 0.7321 - val_acc: 0.8338\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.84030\n",
      "Epoch 206/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1858 - acc: 0.9359 - val_loss: 0.7762 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.84030\n",
      "Epoch 207/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1862 - acc: 0.9342 - val_loss: 0.7707 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.84030\n",
      "Epoch 208/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1821 - acc: 0.9365 - val_loss: 0.7767 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.84030\n",
      "Epoch 209/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1813 - acc: 0.9365 - val_loss: 0.7624 - val_acc: 0.8262\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.84030\n",
      "Epoch 210/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1784 - acc: 0.9371 - val_loss: 0.7462 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.84030\n",
      "Epoch 211/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1806 - acc: 0.9366 - val_loss: 0.7646 - val_acc: 0.8285\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.84030\n",
      "Epoch 212/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1802 - acc: 0.9357 - val_loss: 0.7949 - val_acc: 0.8223\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.84030\n",
      "Epoch 213/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1768 - acc: 0.9380 - val_loss: 0.7857 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.84030\n",
      "Epoch 214/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1776 - acc: 0.9387 - val_loss: 0.7476 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.84030\n",
      "Epoch 215/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1770 - acc: 0.9382 - val_loss: 0.8370 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.84030\n",
      "Epoch 216/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1753 - acc: 0.9382 - val_loss: 0.7637 - val_acc: 0.8246\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.84030\n",
      "Epoch 217/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1708 - acc: 0.9397 - val_loss: 0.8221 - val_acc: 0.8166\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.84030\n",
      "Epoch 218/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1745 - acc: 0.9386 - val_loss: 0.7363 - val_acc: 0.8339\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.84030\n",
      "Epoch 219/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1736 - acc: 0.9394 - val_loss: 0.8018 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.84030\n",
      "Epoch 220/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1751 - acc: 0.9394 - val_loss: 0.7979 - val_acc: 0.8202\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.84030\n",
      "Epoch 221/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1713 - acc: 0.9394 - val_loss: 0.8411 - val_acc: 0.8136\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.84030\n",
      "Epoch 222/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1705 - acc: 0.9404 - val_loss: 0.7410 - val_acc: 0.8327\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.84030\n",
      "Epoch 223/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1714 - acc: 0.9408 - val_loss: 0.7607 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.84030\n",
      "Epoch 224/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1680 - acc: 0.9407 - val_loss: 0.7534 - val_acc: 0.8332\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.84030\n",
      "Epoch 225/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1694 - acc: 0.9398 - val_loss: 0.7196 - val_acc: 0.8388\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.84030\n",
      "Epoch 226/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1677 - acc: 0.9408 - val_loss: 0.8233 - val_acc: 0.8186\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.84030\n",
      "Epoch 227/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1713 - acc: 0.9392 - val_loss: 0.8076 - val_acc: 0.8211\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.84030\n",
      "Epoch 228/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1682 - acc: 0.9404 - val_loss: 0.7372 - val_acc: 0.8357\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.84030\n",
      "Epoch 229/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1693 - acc: 0.9405 - val_loss: 0.7725 - val_acc: 0.8298\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.84030\n",
      "Epoch 230/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1682 - acc: 0.9412 - val_loss: 1.1493 - val_acc: 0.7685\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.84030\n",
      "Epoch 231/250\n",
      "52000/52000 [==============================] - 171s 3ms/step - loss: 0.1630 - acc: 0.9423 - val_loss: 0.6878 - val_acc: 0.8387\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.84030\n",
      "Epoch 232/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1655 - acc: 0.9414 - val_loss: 0.7754 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.84030\n",
      "Epoch 233/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1635 - acc: 0.9414 - val_loss: 0.8525 - val_acc: 0.8219\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.84030\n",
      "Epoch 234/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1630 - acc: 0.9430 - val_loss: 0.7420 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.84030\n",
      "Epoch 235/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1638 - acc: 0.9418 - val_loss: 0.8773 - val_acc: 0.8089\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.84030\n",
      "Epoch 236/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1646 - acc: 0.9412 - val_loss: 0.8316 - val_acc: 0.8186\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.84030\n",
      "Epoch 237/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1650 - acc: 0.9419 - val_loss: 0.8536 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.84030\n",
      "Epoch 238/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1595 - acc: 0.9452 - val_loss: 0.7539 - val_acc: 0.8310\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.84030\n",
      "Epoch 239/250\n",
      "52000/52000 [==============================] - 172s 3ms/step - loss: 0.1654 - acc: 0.9420 - val_loss: 0.7769 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.84030\n",
      "Epoch 240/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1602 - acc: 0.9441 - val_loss: 0.7255 - val_acc: 0.8384\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.84030\n",
      "Epoch 241/250\n",
      "52000/52000 [==============================] - 173s 3ms/step - loss: 0.1555 - acc: 0.9454 - val_loss: 0.7919 - val_acc: 0.8276\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.84030\n",
      "Epoch 242/250\n",
      "52000/52000 [==============================] - 176s 3ms/step - loss: 0.1567 - acc: 0.9451 - val_loss: 0.7581 - val_acc: 0.8370\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.84030\n",
      "Epoch 243/250\n",
      "22080/52000 [===========>..................] - ETA: 1:33 - loss: 0.1590 - acc: 0.9438"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "DNST_CIFAR10_monimoy_changes_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
